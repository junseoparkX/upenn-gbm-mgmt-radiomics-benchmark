{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b1597ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged rows with usable MGMT: 59\n",
      "X shape: (59, 135) y shape: (59,)\n",
      "Class balance (y): {0: 37, 1: 22}\n"
     ]
    }
   ],
   "source": [
    "# Build X (radiomics features) and y (MGMT label)\n",
    "# - Uses segmentation-based radiomics CSV (CaPTk_segm_*.csv)\n",
    "# - Uses UPENN-GBM_clinical_info_v2.1.csv for labels\n",
    "# - Does NOT save anything\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# If you run this from: .../MRI/code/\n",
    "FEATURES_PATH = \"../preprocessing_data/Radiomic_Features_CaPTk_segm_FLAIR_NC.csv\"\n",
    "CLINICAL_PATH = \"../preprocessing_data/UPENN-GBM_clinical_info_v2.1.csv\"\n",
    "\n",
    "# 1) Load\n",
    "df_feat = pd.read_csv(FEATURES_PATH)   # has: SubjectID + many feature columns\n",
    "df_clin = pd.read_csv(CLINICAL_PATH)  # has: ID, MGMT, ...\n",
    "\n",
    "# 2) Merge on subject key\n",
    "df = df_feat.merge(\n",
    "    df_clin[[\"ID\", \"MGMT\"]],\n",
    "    left_on=\"SubjectID\",\n",
    "    right_on=\"ID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# 3) Build y from MGMT\n",
    "# Keep only the two clean classes; drop \"Not Available\" and \"Indeterminate\"\n",
    "df = df[df[\"MGMT\"].isin([\"Methylated\", \"Unmethylated\"])].copy()\n",
    "\n",
    "y = df[\"MGMT\"].map({\"Methylated\": 1, \"Unmethylated\": 0}).astype(int)\n",
    "\n",
    "# 4) Build X from numeric feature columns\n",
    "# Drop identifier/label columns\n",
    "X_all = df.drop(columns=[\"SubjectID\", \"ID\", \"MGMT\"], errors=\"ignore\")\n",
    "\n",
    "# Keep only numeric columns (radiomics should be numeric)\n",
    "X = X_all.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Optional: remove all-NaN or constant columns (safe cleanup)\n",
    "X = X.dropna(axis=1, how=\"all\")\n",
    "X = X.loc[:, X.nunique(dropna=True) > 1]\n",
    "\n",
    "print(\"Merged rows with usable MGMT:\", len(df))\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"Class balance (y):\", y.value_counts().to_dict())\n",
    "\n",
    "# Now you have:\n",
    "#   X: pandas DataFrame\n",
    "#   y: pandas Series (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11825800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Split verification ===\n",
      "Total rows in current df: 59\n",
      "Test IDs requested (file): 30\n",
      "Overlap (test IDs found in this df): 12\n",
      "Missing test IDs (not present in this df): 18\n",
      "First 10 missing: ['UPENN-GBM-00301_11', 'UPENN-GBM-00381_11', 'UPENN-GBM-00405_11', 'UPENN-GBM-00424_11', 'UPENN-GBM-00445_11', 'UPENN-GBM-00446_11', 'UPENN-GBM-00447_11', 'UPENN-GBM-00452_11', 'UPENN-GBM-00455_11', 'UPENN-GBM-00458_11']\n",
      "\n",
      "=== Split sizes ===\n",
      "Train: (47, 135) (47,)\n",
      "Test : (12, 135) (12,)\n",
      "Test class balance: {1: 6, 0: 6}\n"
     ]
    }
   ],
   "source": [
    "# Next step: verify test IDs exist in this dataframe, then split into train/test\n",
    "# - Assumes you already ran the previous block and have: df, X, y\n",
    "# - Assumes you already created a fixed test-id CSV (15/15) somewhere\n",
    "# - Does NOT save anything\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your fixed test IDs (edit if needed)\n",
    "TEST_IDS_PATH = \"../data/test_ids_core10_plus_extra20_seed340.csv\"  # columns: ID, y_mgmt (or ID,y)\n",
    "\n",
    "# 1) Load test IDs\n",
    "test_ids_df = pd.read_csv(TEST_IDS_PATH)\n",
    "\n",
    "# Robustly pick the ID column name from the test file\n",
    "if \"ID\" in test_ids_df.columns:\n",
    "    test_ids = set(test_ids_df[\"ID\"].astype(str))\n",
    "elif \"SubjectID\" in test_ids_df.columns:\n",
    "    test_ids = set(test_ids_df[\"SubjectID\"].astype(str))\n",
    "else:\n",
    "    raise ValueError(f\"Test IDs file must contain 'ID' or 'SubjectID'. Columns: {list(test_ids_df.columns)}\")\n",
    "\n",
    "# 2) Check overlap with current df (this df only contains subjects in FLAIR_NC + usable MGMT)\n",
    "df_ids = df[\"ID\"].astype(str)\n",
    "in_test_mask = df_ids.isin(test_ids)\n",
    "\n",
    "n_total = len(df)\n",
    "n_overlap = int(in_test_mask.sum())\n",
    "print(\"=== Split verification ===\")\n",
    "print(\"Total rows in current df:\", n_total)\n",
    "print(\"Test IDs requested (file):\", len(test_ids))\n",
    "print(\"Overlap (test IDs found in this df):\", n_overlap)\n",
    "\n",
    "# Optional: show missing IDs (from test list that are NOT in this df)\n",
    "missing = sorted(list(test_ids - set(df_ids)))\n",
    "print(\"Missing test IDs (not present in this df):\", len(missing))\n",
    "print(\"First 10 missing:\", missing[:10])\n",
    "\n",
    "# 3) Split (NOTE: for this specific feature file, test size may be <30 due to missing IDs)\n",
    "X_test = X.loc[in_test_mask].copy()\n",
    "y_test = y.loc[in_test_mask].copy()\n",
    "\n",
    "X_train = X.loc[~in_test_mask].copy()\n",
    "y_train = y.loc[~in_test_mask].copy()\n",
    "\n",
    "print(\"\\n=== Split sizes ===\")\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Test :\", X_test.shape, y_test.shape)\n",
    "print(\"Test class balance:\", y_test.value_counts().to_dict())\n",
    "\n",
    "# Now you have:\n",
    "#   X_train, y_train, X_test, y_test\n",
    "# ready for CV on train and final evaluation on test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ef463b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== L1 Linear SVM feature filtering ===\n",
      "Original #features: 135\n",
      "#features with |coef| > 1e-06: 30\n",
      "\n",
      "Top features (up to 20):\n",
      "FLAIR_NC_GLCM_Bins-16_Radius-1_Contrast  |coef|=2.877977\n",
      "FLAIR_NC_GLCM_Bins-16_Radius-1_Homogeneity  |coef|=2.249036\n",
      "FLAIR_NC_Morphologic_EllipseDiameter_Axis-2  |coef|=1.136253\n",
      "FLAIR_NC_Intensity_Range  |coef|=1.039819\n",
      "FLAIR_NC_GLRLM_Bins-16_Radius-1_ShortRunEmphasis  |coef|=0.988838\n",
      "FLAIR_NC_GLSZM_Bins-16_Radius-1_LowGreyLevelEmphasis  |coef|=0.922961\n",
      "FLAIR_NC_Morphologic_Elongation  |coef|=0.668313\n",
      "FLAIR_NC_Histogram_Bins-16_Bins-16_Bin-8_Frequency  |coef|=0.652301\n",
      "FLAIR_NC_Histogram_Bins-16_Bins-16_Bin-10_Probability  |coef|=0.597190\n",
      "FLAIR_NC_GLSZM_Bins-16_Radius-1_SmallZoneHighGreyLevelEmphasis  |coef|=0.539367\n",
      "FLAIR_NC_Histogram_Bins-16_Bins-16_Mode  |coef|=0.518078\n",
      "FLAIR_NC_Intensity_MeanAbsoluteDeviation  |coef|=0.481774\n",
      "FLAIR_NC_Intensity_Mode  |coef|=0.463164\n",
      "FLAIR_NC_Morphologic_Flatness  |coef|=0.452686\n",
      "FLAIR_NC_Histogram_Bins-16_Bins-16_Bin-1_Probability  |coef|=0.439474\n",
      "FLAIR_NC_GLSZM_Bins-16_Radius-1_LargeZoneLowGreyLevelEmphasis  |coef|=0.401396\n",
      "FLAIR_NC_Histogram_Bins-16_Bins-16_MeanAbsoluteDeviation  |coef|=0.273026\n",
      "FLAIR_NC_Histogram_Bins-16_Bins-16_Bin-3_Probability  |coef|=0.236553\n",
      "FLAIR_NC_Morphologic_Roundness  |coef|=0.229731\n",
      "FLAIR_NC_GLSZM_Bins-16_Radius-1_SmallZoneEmphasis  |coef|=0.191174\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# 1) L1 Linear SVM training (SVM은 scaling 거의 필수)\n",
    "svm_l1 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svc\", LinearSVC(\n",
    "        penalty=\"l1\",\n",
    "        dual=False,\n",
    "        C=15,            # 필요하면 조절 (작을수록 더 많이 줄어듦)\n",
    "        random_state=340,\n",
    "        max_iter=20000\n",
    "    ))\n",
    "])\n",
    "\n",
    "svm_l1.fit(X_train, y_train)\n",
    "\n",
    "# 2) Get coefficients and select non-zero (or |coef|>eps)\n",
    "eps = 1e-6\n",
    "svc = svm_l1.named_steps[\"svc\"]\n",
    "coefs = svc.coef_  # (n_classes, n_features) or (1, n_features)\n",
    "\n",
    "# multiclass면 어느 클래스든 0 아니면 선택\n",
    "abs_w = np.max(np.abs(coefs), axis=0) if coefs.ndim == 2 else np.abs(coefs)\n",
    "selected_mask = abs_w > eps\n",
    "selected_features = X_train.columns[selected_mask].tolist()\n",
    "\n",
    "# 3) Filter X_train / X_test to selected features\n",
    "X_train_sel = X_train[selected_features].copy()\n",
    "X_test_sel  = X_test[selected_features].copy()\n",
    "\n",
    "# 4) Report how many features remain\n",
    "print(\"=== L1 Linear SVM feature filtering ===\")\n",
    "print(\"Original #features:\", X_train.shape[1])\n",
    "print(f\"#features with |coef| > {eps}:\", len(selected_features))\n",
    "\n",
    "# Optional: show top-20 by |coef| (selected ones)\n",
    "top_idx = np.argsort(abs_w)[::-1]\n",
    "top_selected = [i for i in top_idx if selected_mask[i]][:20]\n",
    "\n",
    "print(\"\\nTop features (up to 20):\")\n",
    "for i in top_selected:\n",
    "    print(f\"{X_train.columns[i]}  |coef|={abs_w[i]:.6f}\")\n",
    "\n",
    "# Now use:\n",
    "#   X_train_sel, y_train\n",
    "#   X_test_sel,  y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce2f3623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: C:\\Users\\junse\\Documents\\research\\UPENN-GBM\\MRI\\Net\n",
      "Files: X_train.csv y_train.csv X_test.csv y_test.csv\n",
      "Shapes: X_train (47, 30) y_train 47 X_test (12, 30) y_test 12\n"
     ]
    }
   ],
   "source": [
    "# Save filtered (selected) features + labels into MRI/Net/\n",
    "# Creates 4 files:\n",
    "#   - Net/X_train.csv\n",
    "#   - Net/y_train.csv\n",
    "#   - Net/X_test.csv\n",
    "#   - Net/y_test.csv\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Assumes these exist from previous steps:\n",
    "#   X_train_sel, y_train, X_test_sel, y_test\n",
    "\n",
    "# Resolve output directory robustly (works regardless of current working directory)\n",
    "# If running from MRI/code/, this will point to MRI/Net/\n",
    "base_dir = Path(\"..\").resolve()   # MRI/\n",
    "net_dir = base_dir / \"Net\"\n",
    "net_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure indices align and save without index\n",
    "X_train_sel.to_csv(net_dir / \"X_train.csv\", index=False)\n",
    "pd.Series(y_train).reset_index(drop=True).to_csv(net_dir / \"y_train.csv\", index=False, header=[\"y\"])\n",
    "\n",
    "X_test_sel.to_csv(net_dir / \"X_test.csv\", index=False)\n",
    "pd.Series(y_test).reset_index(drop=True).to_csv(net_dir / \"y_test.csv\", index=False, header=[\"y\"])\n",
    "\n",
    "print(\"Saved to:\", net_dir)\n",
    "print(\"Files:\",\n",
    "      (net_dir / \"X_train.csv\").name,\n",
    "      (net_dir / \"y_train.csv\").name,\n",
    "      (net_dir / \"X_test.csv\").name,\n",
    "      (net_dir / \"y_test.csv\").name)\n",
    "print(\"Shapes:\",\n",
    "      \"X_train\", X_train_sel.shape,\n",
    "      \"y_train\", len(y_train),\n",
    "      \"X_test\", X_test_sel.shape,\n",
    "      \"y_test\", len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
